{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revise from https://dp.tdhopper.com/nonparametric-lda/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.2f'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%precision 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonparametric Latent Dirichlet Allocation\n",
    "\n",
    "_Latent Dirichlet Allocation_is a [generative](https://en.wikipedia.org/wiki/Generative_model) model for topic modeling. Given a collection of documents, an LDA inference algorithm attempts to determined (in an unsupervised manner) the topics discussed in the documents. It makes the assumption that each document is generated by a probability model, and, when doing inference, we try to find the parameters that best fit the model (as well as unseen/latent variables generated by the model). If you are unfamiliar with LDA, Edwin Chen has a [friendly introduction](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/) you should read.\n",
    "\n",
    "\n",
    "Because LDA is a _generative_ model, we can simulate the construction of documents by forward-sampling from the model. The generative algorithm is as follows (following [Heinrich](http://www.arbylon.net/publications/text-est.pdf)):\n",
    "\n",
    "* for each topic $k\\in [1,K]$ do\n",
    "    * sample term distribution for topic $\\overrightarrow \\phi_k \\sim \\text{Dir}(\\overrightarrow \\beta)$\n",
    "* for each document $m\\in [1, M]$ do\n",
    "    * sample topic distribution for document $\\overrightarrow\\theta_m\\sim \\text{Dir}(\\overrightarrow\\alpha)$\n",
    "    * sample document length $N_m\\sim\\text{Pois}(\\xi)$\n",
    "    * for all words $n\\in [1, N_m]$ in document $m$ do\n",
    "        * sample topic index $z_{m,n}\\sim\\text{Mult}(\\overrightarrow\\theta_m)$\n",
    "        * sample term for word $w_{m,n}\\sim\\text{Mult}(\\overrightarrow\\phi_{z_{m,n}})$\n",
    "        \n",
    "You can implement this with [a little bit of code](https://gist.github.com/tdhopper/521006b60e1311d45509) and start to simulate documents.\n",
    "\n",
    "In LDA, we assume each word in the document is generated by a two-step process:\n",
    "\n",
    "1. Sample a topic from the topic distribution for the document.\n",
    "2. Sample a word from the term distribution from the topic. \n",
    "\n",
    "When we fit the LDA model to a given text corpus with an inference algorithm, our primary objective is to find the set of topic distributions $\\underline \\Theta$, term distributions $\\underline \\Phi$ that generated the documents, and latent topic indices $z_{m,n}$ for each word.\n",
    "\n",
    "To run the generative model, we need to specify each of these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V0', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99']\n"
     ]
    }
   ],
   "source": [
    "vocabulary=[]\n",
    "for ii in range(100):\n",
    "    vocabulary.append('V'+ str(ii))\n",
    "print(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary = ['A', 'B', 'C', \"D\", \"E\", \"F\"]\n",
    "num_terms = len(vocabulary)\n",
    "num_topics = 2 # K\n",
    "num_documents = 5 # M\n",
    "mean_document_length = 5000 # xi\n",
    "term_dirichlet_parameter = 1 # beta\n",
    "topic_dirichlet_parameter = 1 # alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term distribution vector $\\underline\\Phi$ is a collection of samples from a Dirichlet distribution. This describes how our 3 terms are distributed across each of the two topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cwu/opt/anaconda3/envs/HDP/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import dirichlet, poisson\n",
    "from numpy import round\n",
    "from collections import defaultdict\n",
    "from random import choice as stl_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_dirichlet_vector = num_terms * [term_dirichlet_parameter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_dirichlet_vector = num_terms * [term_dirichlet_parameter]\n",
    "term_distributions = dirichlet(term_dirichlet_vector, 10).rvs(size=num_topics)  # for each topic\n",
    "#print(term_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document corresponds to a categorical distribution across this distribution of topics (in this case, a 2-dimensional categorical distribution). This categorical distribution is a _distribution of distributions_; we could look at it as a Dirichlet process!\n",
    "\n",
    "The base base distribution of our Dirichlet process is a uniform distribution of topics (remember, topics are term distributions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 5029 topic: [0.01, 0.02, 0.0, 0.01, 0.02, 0.02, 0.01, 0.0, 0.01, 0.01, 0.0, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.03, 0.01, 0.0, 0.02, 0.0, 0.02, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.0, 0.01, 0.03, 0.03, 0.01, 0.01, 0.0, 0.05, 0.01, 0.02, 0.01, 0.03, 0.01, 0.02, 0.0, 0.01, 0.01, 0.0, 0.01, 0.0, 0.01, 0.02, 0.0, 0.02, 0.01, 0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01, 0.02, 0.01, 0.01, 0.0, 0.02, 0.01, 0.02, 0.02, 0.01, 0.02, 0.01, 0.0, 0.02, 0.0, 0.01, 0.01, 0.01, 0.02, 0.0, 0.0, 0.02, 0.02, 0.01, 0.01, 0.0, 0.01, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.01, 0.02, 0.01, 0.0]\n",
      "count: 4971 topic: [0.02, 0.0, 0.01, 0.02, 0.01, 0.0, 0.0, 0.02, 0.0, 0.0, 0.01, 0.03, 0.0, 0.01, 0.02, 0.01, 0.01, 0.0, 0.03, 0.01, 0.01, 0.0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01, 0.03, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.01, 0.02, 0.0, 0.02, 0.0, 0.02, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.02, 0.0, 0.01, 0.03, 0.05, 0.01, 0.02, 0.0, 0.01, 0.03, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01, 0.0, 0.0, 0.04, 0.0, 0.01, 0.04, 0.01, 0.0, 0.01]\n"
     ]
    }
   ],
   "source": [
    "base_distribution = lambda: stl_choice(term_distributions)\n",
    "# A sample from base_distribution is a distribution over terms\n",
    "# Each of our two topics has equal probability\n",
    "from collections import Counter\n",
    "for topic, count in Counter([tuple(base_distribution()) for _ in range(10000)]).most_common():\n",
    "    print(\"count:\", count, \"topic:\", [round(prob, 2) for prob in topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a sample from a Dirichlet process is a distribution that approximates (but varies from) the base distribution. In this case, a sample from the Dirichlet process will be a distribution over topics that varies from the uniform distribution we provided as a base. If we use the stick-breaking metaphor, we are effectively breaking a stick one time and the size of each portion corresponds to the proportion of a topic in the document.\n",
    "\n",
    "To construct a sample from the DP, we need to [again define our DP class](/dirichlet-distribution/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "from numpy.random import choice\n",
    "\n",
    "class DirichletProcessSample():\n",
    "    def __init__(self, base_measure, alpha):\n",
    "        self.base_measure = base_measure\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.cache = []\n",
    "        self.weights = []\n",
    "        self.total_stick_used = 0.\n",
    "\n",
    "    def __call__(self):\n",
    "        remaining = 1.0 - self.total_stick_used\n",
    "        i = DirichletProcessSample.roll_die(self.weights + [remaining])\n",
    "        #print(\"i\")\n",
    "        #print(i)\n",
    "        if i is not None and i < len(self.weights) :\n",
    "            #print(\"if\")\n",
    "            #print(\"cache\")\n",
    "            #print(self.cache)\n",
    "            return self.cache[i]\n",
    "        else:\n",
    "            #print(\"else\")\n",
    "            stick_piece = beta(1, self.alpha).rvs() * remaining\n",
    "            self.total_stick_used += stick_piece\n",
    "            self.weights.append(stick_piece)\n",
    "            new_value = self.base_measure()\n",
    "            self.cache.append(new_value)\n",
    "            #print(\"cache\")\n",
    "            #print(self.cache)\n",
    "            return new_value\n",
    "      \n",
    "    @staticmethod \n",
    "    def roll_die(weights):\n",
    "        if weights:\n",
    "            #print(\"weights\")\n",
    "            #print(weights)\n",
    "            return choice(range(len(weights)), p=weights)\n",
    "        else:\n",
    "            #print(\"None\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document, we will draw a topic distribution from the Dirichlet process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distribution = DirichletProcessSample(base_measure=base_distribution, \n",
    "                                          alpha=topic_dirichlet_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample from this _topic_ distribution is a _distribution over terms_. However, unlike our base distribution which returns each term distribution with equal probability, the topics will be unevenly weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 6056 topic: [0.02, 0.0, 0.01, 0.02, 0.01, 0.0, 0.0, 0.02, 0.0, 0.0, 0.01, 0.03, 0.0, 0.01, 0.02, 0.01, 0.01, 0.0, 0.03, 0.01, 0.01, 0.0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01, 0.03, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.01, 0.02, 0.0, 0.02, 0.0, 0.02, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.02, 0.0, 0.01, 0.03, 0.05, 0.01, 0.02, 0.0, 0.01, 0.03, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01, 0.0, 0.0, 0.04, 0.0, 0.01, 0.04, 0.01, 0.0, 0.01]\n",
      "count: 3944 topic: [0.01, 0.02, 0.0, 0.01, 0.02, 0.02, 0.01, 0.0, 0.01, 0.01, 0.0, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.03, 0.01, 0.0, 0.02, 0.0, 0.02, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.0, 0.01, 0.03, 0.03, 0.01, 0.01, 0.0, 0.05, 0.01, 0.02, 0.01, 0.03, 0.01, 0.02, 0.0, 0.01, 0.01, 0.0, 0.01, 0.0, 0.01, 0.02, 0.0, 0.02, 0.01, 0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01, 0.02, 0.01, 0.01, 0.0, 0.02, 0.01, 0.02, 0.02, 0.01, 0.02, 0.01, 0.0, 0.02, 0.0, 0.01, 0.01, 0.01, 0.02, 0.0, 0.0, 0.02, 0.02, 0.01, 0.01, 0.0, 0.01, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.01, 0.02, 0.01, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for topic, count in Counter([tuple(topic_distribution()) for _ in range(10000)]).most_common():\n",
    "    print(\"count:\", count, \"topic:\", [round(prob, 2) for prob in topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the topic distribution uneven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate each word in the document, we draw a sample topic from the topic distribution, and then a term from the term distribution (topic). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_index = defaultdict(list)\n",
    "documents = defaultdict(list)\n",
    "\n",
    "for doc in range(num_documents):\n",
    "    topic_distribution_rvs = DirichletProcessSample(base_measure=base_distribution, \n",
    "                                                    alpha=topic_dirichlet_parameter)\n",
    "    document_length = poisson(mean_document_length).rvs()\n",
    "    for word in range(document_length):\n",
    "        topic_distribution = topic_distribution_rvs()\n",
    "        topic_index[doc].append(tuple(topic_distribution))\n",
    "        documents[doc].append(choice(vocabulary, p=topic_distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the documents we generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word\n",
      "0    V0\n",
      "1    V1\n",
      "2    V2\n",
      "3    V3\n",
      "4    V4\n",
      "..  ...\n",
      "95  V95\n",
      "96  V96\n",
      "97  V97\n",
      "98  V98\n",
      "99  V99\n",
      "\n",
      "[100 rows x 1 columns]\n",
      "      0    1   2   3    4   5   6   7   8   9   ...  90  91  92   93  94  95  \\\n",
      "doc1  63   88  23  47   83  54  46  17  19  52  ...  13  80   4   56  47  28   \n",
      "doc2  66    4  62  74   44  19  14  86  13   3  ...  36  16   3  194  21  35   \n",
      "doc3  55  102  11  46  104  80  54   1  27  55  ...   5  73   3   30  54  24   \n",
      "doc4  55   74  26  58   62  46  31  30  25  30  ...  11  47   5   83  48  32   \n",
      "doc5  62   52  33  61   66  42  45  41  17  27  ...  20  37   5  114  26  43   \n",
      "\n",
      "       96  97  98  99  \n",
      "doc1   86  91  14  12  \n",
      "doc2  212  35  20  50  \n",
      "doc3   83  79  25  10  \n",
      "doc4  135  93  33  26  \n",
      "doc5  157  73  22  24  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "## data to dataframe\n",
    "df_all=pd.DataFrame(vocabulary, columns=[\"word\"])\n",
    "i=0\n",
    "print(df_all)\n",
    "for doc in documents.values():\n",
    "    i+=1\n",
    "    #counter=Counter(doc)\n",
    "    #scounter=sorted(counter.items())\n",
    "    #print(scounter)\n",
    "    scount=[]\n",
    "    for vv in vocabulary:\n",
    "        scount.append(doc.count(vv))\n",
    "    #df=pd.DataFrame(scounter, columns=['word', \"freq\"])\n",
    "    df=pd.DataFrame(list(zip(vocabulary, scount)), columns=['word', \"freq\"])\n",
    "    if(all(df['word']==df_all['word'])):\n",
    "        df_all['doc'+str(i)]=df['freq']\n",
    "    else:\n",
    "        print(\"not match\")\n",
    "    \n",
    "    #print((sorted(counter.items())[0][1]))\n",
    "    \n",
    "df_all2=df_all.iloc[:,range(1,num_documents+1)].T\n",
    "print(df_all2)\n",
    "df_all2.to_csv(f\"~/test_data_topic_model_{num_terms}voc_{num_topics}topics_{num_documents}docs_{mean_document_length}words_beta{term_dirichlet_parameter}_alpha{topic_dirichlet_parameter}.csv\"\n",
    "              ,header=False)\n",
    "               \n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how each topic (term-distribution) is distributed across the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: 0\n",
      "      count: 4909 topic: [0.02, 0.0, 0.01, 0.02, 0.01, 0.0, 0.0, 0.02, 0.0, 0.0, 0.01, 0.03, 0.0, 0.01, 0.02, 0.01, 0.01, 0.0, 0.03, 0.01, 0.01, 0.0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01, 0.03, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.01, 0.02, 0.0, 0.02, 0.0, 0.02, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.02, 0.0, 0.01, 0.03, 0.05, 0.01, 0.02, 0.0, 0.01, 0.03, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01, 0.0, 0.0, 0.04, 0.0, 0.01, 0.04, 0.01, 0.0, 0.01]\n",
      "      count: 81 topic: [0.01, 0.02, 0.0, 0.01, 0.02, 0.02, 0.01, 0.0, 0.01, 0.01, 0.0, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.03, 0.01, 0.0, 0.02, 0.0, 0.02, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.0, 0.01, 0.03, 0.03, 0.01, 0.01, 0.0, 0.05, 0.01, 0.02, 0.01, 0.03, 0.01, 0.02, 0.0, 0.01, 0.01, 0.0, 0.01, 0.0, 0.01, 0.02, 0.0, 0.02, 0.01, 0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01, 0.02, 0.01, 0.01, 0.0, 0.02, 0.01, 0.02, 0.02, 0.01, 0.02, 0.01, 0.0, 0.02, 0.0, 0.01, 0.01, 0.01, 0.02, 0.0, 0.0, 0.02, 0.02, 0.01, 0.01, 0.0, 0.01, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.01, 0.02, 0.01, 0.0]\n",
      "Doc: 1\n",
      "      count: 4950 topic: [0.02, 0.0, 0.01, 0.02, 0.01, 0.0, 0.0, 0.02, 0.0, 0.0, 0.01, 0.03, 0.0, 0.01, 0.02, 0.01, 0.01, 0.0, 0.03, 0.01, 0.01, 0.0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01, 0.03, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.01, 0.02, 0.0, 0.02, 0.0, 0.02, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.02, 0.0, 0.01, 0.03, 0.05, 0.01, 0.02, 0.0, 0.01, 0.03, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01, 0.0, 0.0, 0.04, 0.0, 0.01, 0.04, 0.01, 0.0, 0.01]\n",
      "      count: 72 topic: [0.01, 0.02, 0.0, 0.01, 0.02, 0.02, 0.01, 0.0, 0.01, 0.01, 0.0, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.03, 0.01, 0.0, 0.02, 0.0, 0.02, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.0, 0.01, 0.03, 0.03, 0.01, 0.01, 0.0, 0.05, 0.01, 0.02, 0.01, 0.03, 0.01, 0.02, 0.0, 0.01, 0.01, 0.0, 0.01, 0.0, 0.01, 0.02, 0.0, 0.02, 0.01, 0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01, 0.02, 0.01, 0.01, 0.0, 0.02, 0.01, 0.02, 0.02, 0.01, 0.02, 0.01, 0.0, 0.02, 0.0, 0.01, 0.01, 0.01, 0.02, 0.0, 0.0, 0.02, 0.02, 0.01, 0.01, 0.0, 0.01, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.01, 0.02, 0.01, 0.0]\n",
      "Doc: 2\n",
      "      count: 4293 topic: [0.02, 0.0, 0.01, 0.02, 0.01, 0.0, 0.0, 0.02, 0.0, 0.0, 0.01, 0.03, 0.0, 0.01, 0.02, 0.01, 0.01, 0.0, 0.03, 0.01, 0.01, 0.0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01, 0.03, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.01, 0.02, 0.0, 0.02, 0.0, 0.02, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.02, 0.0, 0.01, 0.03, 0.05, 0.01, 0.02, 0.0, 0.01, 0.03, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01, 0.0, 0.0, 0.04, 0.0, 0.01, 0.04, 0.01, 0.0, 0.01]\n",
      "      count: 670 topic: [0.01, 0.02, 0.0, 0.01, 0.02, 0.02, 0.01, 0.0, 0.01, 0.01, 0.0, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.03, 0.01, 0.0, 0.02, 0.0, 0.02, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.0, 0.01, 0.03, 0.03, 0.01, 0.01, 0.0, 0.05, 0.01, 0.02, 0.01, 0.03, 0.01, 0.02, 0.0, 0.01, 0.01, 0.0, 0.01, 0.0, 0.01, 0.02, 0.0, 0.02, 0.01, 0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01, 0.02, 0.01, 0.01, 0.0, 0.02, 0.01, 0.02, 0.02, 0.01, 0.02, 0.01, 0.0, 0.02, 0.0, 0.01, 0.01, 0.01, 0.02, 0.0, 0.0, 0.02, 0.02, 0.01, 0.01, 0.0, 0.01, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.01, 0.02, 0.01, 0.0]\n",
      "Doc: 3\n",
      "      count: 4424 topic: [0.01, 0.02, 0.0, 0.01, 0.02, 0.02, 0.01, 0.0, 0.01, 0.01, 0.0, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.03, 0.01, 0.0, 0.02, 0.0, 0.02, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.0, 0.01, 0.03, 0.03, 0.01, 0.01, 0.0, 0.05, 0.01, 0.02, 0.01, 0.03, 0.01, 0.02, 0.0, 0.01, 0.01, 0.0, 0.01, 0.0, 0.01, 0.02, 0.0, 0.02, 0.01, 0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01, 0.02, 0.01, 0.01, 0.0, 0.02, 0.01, 0.02, 0.02, 0.01, 0.02, 0.01, 0.0, 0.02, 0.0, 0.01, 0.01, 0.01, 0.02, 0.0, 0.0, 0.02, 0.02, 0.01, 0.01, 0.0, 0.01, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.01, 0.02, 0.01, 0.0]\n",
      "      count: 629 topic: [0.02, 0.0, 0.01, 0.02, 0.01, 0.0, 0.0, 0.02, 0.0, 0.0, 0.01, 0.03, 0.0, 0.01, 0.02, 0.01, 0.01, 0.0, 0.03, 0.01, 0.01, 0.0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01, 0.03, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.01, 0.02, 0.0, 0.02, 0.0, 0.02, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.02, 0.0, 0.01, 0.03, 0.05, 0.01, 0.02, 0.0, 0.01, 0.03, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01, 0.0, 0.0, 0.04, 0.0, 0.01, 0.04, 0.01, 0.0, 0.01]\n",
      "Doc: 4\n",
      "      count: 2975 topic: [0.02, 0.0, 0.01, 0.02, 0.01, 0.0, 0.0, 0.02, 0.0, 0.0, 0.01, 0.03, 0.0, 0.01, 0.02, 0.01, 0.01, 0.0, 0.03, 0.01, 0.01, 0.0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01, 0.03, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.01, 0.02, 0.0, 0.02, 0.0, 0.02, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.02, 0.0, 0.01, 0.03, 0.05, 0.01, 0.02, 0.0, 0.01, 0.03, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01, 0.0, 0.0, 0.04, 0.0, 0.01, 0.04, 0.01, 0.0, 0.01]\n",
      "      count: 2010 topic: [0.01, 0.02, 0.0, 0.01, 0.02, 0.02, 0.01, 0.0, 0.01, 0.01, 0.0, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.03, 0.01, 0.0, 0.02, 0.0, 0.02, 0.01, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.0, 0.01, 0.03, 0.03, 0.01, 0.01, 0.0, 0.05, 0.01, 0.02, 0.01, 0.03, 0.01, 0.02, 0.0, 0.01, 0.01, 0.0, 0.01, 0.0, 0.01, 0.02, 0.0, 0.02, 0.01, 0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01, 0.02, 0.01, 0.01, 0.0, 0.02, 0.01, 0.02, 0.02, 0.01, 0.02, 0.01, 0.0, 0.02, 0.0, 0.01, 0.01, 0.01, 0.02, 0.0, 0.0, 0.02, 0.02, 0.01, 0.01, 0.0, 0.01, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.01, 0.02, 0.01, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(Counter(term_dist).most_common() for term_dist in topic_index.values()):\n",
    "    print(\"Doc:\", i)\n",
    "    for topic, count in doc:\n",
    "        print(5*\" \", \"count:\", count, \"topic:\", [round(prob, 2) for prob in topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: 0\n",
      "Doc: 1\n",
      "Doc: 2\n",
      "Doc: 3\n",
      "Doc: 4\n",
      "   [0.016, 0.0, 0.011, 0.015, 0.008, 0.003, 0.002, 0.016, 0.002, 0.001, 0.013, 0.034, 0.0, 0.008, 0.018, 0.01, 0.014, 0.004, 0.027, 0.014, 0.009, 0.002, 0.005, 0.012, 0.006, 0.006, 0.011, 0.008, 0.011, 0.01, 0.018, 0.008, 0.026, 0.004, 0.001, 0.004, 0.001, 0.019, 0.001, 0.011, 0.009, 0.019, 0.002, 0.021, 0.005, 0.015, 0.004, 0.024, 0.004, 0.002, 0.005, 0.001, 0.019, 0.002, 0.005, 0.032, 0.048, 0.007, 0.019, 0.003, 0.01, 0.025, 0.008, 0.01, 0.0, 0.005, 0.001, 0.004, 0.004, 0.016, 0.0, 0.006, 0.004, 0.011, 0.005, 0.0, 0.023, 0.016, 0.023, 0.006, 0.01, 0.008, 0.01, 0.003, 0.004, 0.0, 0.004, 0.003, 0.009, 0.009, 0.007, 0.004, 0.001, 0.042, 0.005, 0.007, 0.041, 0.006, 0.004, 0.008]  \\\n",
      "0                                                938                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "1                                               4945                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "2                                                374                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "3                                               2100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "4                                               2859                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "\n",
      "   [0.011, 0.024, 0.001, 0.008, 0.019, 0.016, 0.011, 0.0, 0.005, 0.01, 0.004, 0.009, 0.014, 0.002, 0.001, 0.005, 0.001, 0.002, 0.027, 0.006, 0.003, 0.018, 0.001, 0.017, 0.008, 0.011, 0.002, 0.003, 0.013, 0.001, 0.004, 0.01, 0.003, 0.006, 0.032, 0.033, 0.01, 0.01, 0.0, 0.048, 0.011, 0.02, 0.01, 0.026, 0.011, 0.015, 0.002, 0.007, 0.015, 0.001, 0.007, 0.002, 0.009, 0.021, 0.001, 0.02, 0.01, 0.011, 0.003, 0.007, 0.004, 0.0, 0.0, 0.008, 0.017, 0.01, 0.006, 0.002, 0.023, 0.011, 0.019, 0.024, 0.012, 0.019, 0.012, 0.0, 0.017, 0.002, 0.008, 0.009, 0.008, 0.023, 0.002, 0.0, 0.024, 0.017, 0.008, 0.007, 0.002, 0.005, 0.001, 0.016, 0.001, 0.003, 0.011, 0.004, 0.013, 0.02, 0.005, 0.001]  \n",
      "0                                               4082                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "1                                                  1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "2                                               4686                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "3                                               2860                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "4                                               2142                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n"
     ]
    }
   ],
   "source": [
    "tmp={}\n",
    "for i, doc in enumerate(sorted(Counter(term_dist).items()) for term_dist in topic_index.values()):\n",
    "    print(\"Doc:\", i)\n",
    "    for jj in topics_all:\n",
    "        doc2=dict(doc)\n",
    "        if tuple(jj) in doc2.keys():\n",
    "            count=doc2[tuple(jj)]\n",
    "            tt=[round(prob, 3) for prob in jj]\n",
    "            if i==0:\n",
    "                tmp[str(tt)]=[count]  \n",
    "            else:\n",
    "                cc=tmp[str(tt)]\n",
    "                cc.append(count)\n",
    "                tmp[str(tt)]=cc\n",
    "        else:\n",
    "            tt=[round(prob, 3) for prob in jj]\n",
    "            if i==0:\n",
    "                tmp[str(tt)]=[0]  \n",
    "            else:\n",
    "                cc=tmp[str(tt)]\n",
    "                cc.append(0)\n",
    "                tmp[str(tt)]=cc\n",
    "            \n",
    "prop_table=pd.DataFrame(tmp)\n",
    "print(prop_table)      \n",
    "prop_table.to_csv(f\"~/test_prop_table_topic_model_{num_terms}voc_{num_topics}topics_{num_documents}docs_{mean_document_length}words_beta{term_dirichlet_parameter}_alpha{topic_dirichlet_parameter}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[0.011, 0.002, 0.037, 0.004, 0.01, 0.062, 0.095, 0.013, 0.038, 0.006, 0.02, 0.05, 0.017, 0.019, 0.001, 0.01, 0.002, 0.008, 0.009, 0.032, 0.001, 0.012, 0.008, 0.022, 0.009, 0.001, 0.046, 0.031, 0.045, 0.012, 0.02, 0.016, 0.02, 0.007, 0.008, 0.001, 0.008, 0.006, 0.018, 0.018, 0.014, 0.007, 0.001, 0.083, 0.009, 0.015, 0.082, 0.013, 0.008, 0.016]</th>\n",
       "      <th>[0.02, 0.046, 0.002, 0.015, 0.036, 0.03, 0.021, 0.001, 0.01, 0.02, 0.008, 0.017, 0.027, 0.004, 0.001, 0.01, 0.003, 0.005, 0.051, 0.011, 0.006, 0.033, 0.003, 0.032, 0.015, 0.022, 0.004, 0.005, 0.025, 0.001, 0.007, 0.019, 0.006, 0.011, 0.061, 0.062, 0.02, 0.018, 0.0, 0.09, 0.02, 0.037, 0.02, 0.05, 0.021, 0.029, 0.004, 0.013, 0.028, 0.003]</th>\n",
       "      <th>[0.033, 0.0, 0.022, 0.031, 0.015, 0.006, 0.005, 0.032, 0.004, 0.002, 0.026, 0.068, 0.0, 0.016, 0.037, 0.021, 0.028, 0.008, 0.055, 0.028, 0.017, 0.003, 0.01, 0.025, 0.013, 0.013, 0.021, 0.016, 0.023, 0.02, 0.036, 0.016, 0.053, 0.009, 0.002, 0.008, 0.003, 0.039, 0.001, 0.022, 0.018, 0.038, 0.005, 0.043, 0.01, 0.031, 0.008, 0.048, 0.009, 0.004]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236</td>\n",
       "      <td>739</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>838</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>212</td>\n",
       "      <td>818</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>410</td>\n",
       "      <td>0</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   [0.011, 0.002, 0.037, 0.004, 0.01, 0.062, 0.095, 0.013, 0.038, 0.006, 0.02, 0.05, 0.017, 0.019, 0.001, 0.01, 0.002, 0.008, 0.009, 0.032, 0.001, 0.012, 0.008, 0.022, 0.009, 0.001, 0.046, 0.031, 0.045, 0.012, 0.02, 0.016, 0.02, 0.007, 0.008, 0.001, 0.008, 0.006, 0.018, 0.018, 0.014, 0.007, 0.001, 0.083, 0.009, 0.015, 0.082, 0.013, 0.008, 0.016]  \\\n",
       "0                                                236                                                                                                                                                                                                                                                                                                          \n",
       "1                                                838                                                                                                                                                                                                                                                                                                          \n",
       "2                                                212                                                                                                                                                                                                                                                                                                          \n",
       "3                                                410                                                                                                                                                                                                                                                                                                          \n",
       "4                                                 26                                                                                                                                                                                                                                                                                                          \n",
       "\n",
       "   [0.02, 0.046, 0.002, 0.015, 0.036, 0.03, 0.021, 0.001, 0.01, 0.02, 0.008, 0.017, 0.027, 0.004, 0.001, 0.01, 0.003, 0.005, 0.051, 0.011, 0.006, 0.033, 0.003, 0.032, 0.015, 0.022, 0.004, 0.005, 0.025, 0.001, 0.007, 0.019, 0.006, 0.011, 0.061, 0.062, 0.02, 0.018, 0.0, 0.09, 0.02, 0.037, 0.02, 0.05, 0.021, 0.029, 0.004, 0.013, 0.028, 0.003]  \\\n",
       "0                                                739                                                                                                                                                                                                                                                                                                    \n",
       "1                                                  0                                                                                                                                                                                                                                                                                                    \n",
       "2                                                818                                                                                                                                                                                                                                                                                                    \n",
       "3                                                  0                                                                                                                                                                                                                                                                                                    \n",
       "4                                                 13                                                                                                                                                                                                                                                                                                    \n",
       "\n",
       "   [0.033, 0.0, 0.022, 0.031, 0.015, 0.006, 0.005, 0.032, 0.004, 0.002, 0.026, 0.068, 0.0, 0.016, 0.037, 0.021, 0.028, 0.008, 0.055, 0.028, 0.017, 0.003, 0.01, 0.025, 0.013, 0.013, 0.021, 0.016, 0.023, 0.02, 0.036, 0.016, 0.053, 0.009, 0.002, 0.008, 0.003, 0.039, 0.001, 0.022, 0.018, 0.038, 0.005, 0.043, 0.01, 0.031, 0.008, 0.048, 0.009, 0.004]  \n",
       "0                                                 35                                                                                                                                                                                                                                                                                                        \n",
       "1                                                123                                                                                                                                                                                                                                                                                                        \n",
       "2                                                 16                                                                                                                                                                                                                                                                                                        \n",
       "3                                                551                                                                                                                                                                                                                                                                                                        \n",
       "4                                                996                                                                                                                                                                                                                                                                                                        "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap: for each document we draw a _sample_ from a Dirichlet _Process_. The base distribution for the Dirichlet process is a categorical distribution over term distributions; we can think of the base distribution as an $n$-sided die where $n$ is the number of topics and each side of the die is a distribution over terms for that topic. By sampling from the Dirichlet process, we are effectively reweighting the sides of the die (changing the distribution of the topics).\n",
    "\n",
    "For each word in the document, we draw a _sample_ (a term distribution) from the distribution (over term distributions) _sampled_ from the Dirichlet process (with a distribution over term distributions as its base measure). Each term distribution uniquely identifies the topic for the word. We can sample from this term distribution to get the word.\n",
    "\n",
    "Given this formulation, we might ask if we can roll an _infinite_ sided die to draw from an unbounded number of topics (term distributions). We can do exactly this with a _Hierarchical_ Dirichlet process. Instead of the base distribution of our Dirichlet process being a _finite_ distribution over topics (term distributions) we will instead make it an infinite Distribution over topics (term distributions) by using yet another Dirichlet process! This base Dirichlet process will have as its base distribution a Dirichlet _distribution_ over terms. \n",
    "\n",
    "We will again draw a _sample_ from a Dirichlet _Process_ for each document. The base distribution for the Dirichlet process is itself a Dirichlet process whose base distribution is a Dirichlet distribution over terms. (Try saying that 5-times fast.) We can think of this as a countably infinite die each side of the die is a distribution over terms for that topic. The sample we draw is a topic (distribution over terms).\n",
    "\n",
    "For each word in the document, we will draw a _sample_ (a term distribution) from the distribution (over term distributions) _sampled_ from the Dirichlet process (with a distribution over term distributions as its base measure). Each term distribution uniquely identifies the topic for the word. We can sample from this term distribution to get the word.\n",
    "\n",
    "These last few paragraphs are confusing! Let's illustrate with code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our documents were generated by an unspecified number of topics, and yet the topics were shared across the 5 documents. This is the power of the hierarchical Dirichlet process!\n",
    "\n",
    "This non-parametric formulation of Latent Dirichlet Allocation was first published by [Yee Whye Teh et al](http://www.cs.berkeley.edu/~jordan/papers/hdp.pdf). \n",
    "\n",
    "Unfortunately, forward sampling is the easy part. Fitting the model on data requires [complex MCMC](http://psiexp.ss.uci.edu/research/papers/sciencetopics.pdf) or [variational inference](http://www.cs.princeton.edu/~chongw/papers/WangPaisleyBlei2011.pdf). There are a [limited](http://www.stats.ox.ac.uk/~teh/software.html) [number](https://github.com/shuyo/iir/blob/master/lda/hdplda2.py) of [implementations](https://github.com/renaud/hdp-faster) [of HDP-LDA](http://www.arbylon.net/resources.html) available, and none of them are great. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDP",
   "language": "python",
   "name": "hdp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "nikola": {
   "slug": "nonparametric-lda",
   "title": "Nonparametric Latent Dirichlet Allocation"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
